#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®¢èªè¾­å…¸è§£æå™¨ - å®Œæ•´ç‰ˆ
è™•ç†ã€Šè‡ºç£å››ç¸£è…”æµ·é™¸è…”å®¢å®¶è©±è¾­å…¸ã€‹HTMLæ ¼å¼
æ”¯æŒï¼šéŸ³ç¯€ä»‹ç´¹ã€ä¸»è©æ¢ã€å–®å­—è©æ¢ã€å¤šå­—è©æ¢
"""

import re
import json
from bs4 import BeautifulSoup, NavigableString
from pathlib import Path


class HakkaParser:
    def __init__(self):
        self.entries = []
        self.current_id = 1
        
    def load_html(self, html_path):
        """è¼‰å…¥HTMLæ–‡ä»¶"""
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return BeautifulSoup(content, 'html.parser')
    
    def extract_text_with_underline(self, element):
        """
        æå–æ–‡æœ¬ï¼Œä¿ç•™ä¸‹åŠƒç·šæ¨™è¨˜
        ä¾‹å¦‚ï¼šc<u>o</u>n â†’ coÌ±n
        """
        if isinstance(element, NavigableString):
            return str(element)
        
        text = ""
        for child in element.children:
            if child.name == 'u':
                # ä¸‹åŠƒç·šçš„å…§å®¹æ·»åŠ çµ„åˆç”¨ä¸‹åŠƒç·šç¬¦è™Ÿ
                text += child.get_text() + '\u0331'  # çµ„åˆç”¨ä¸‹åŠƒç·š
            elif isinstance(child, NavigableString):
                text += str(child)
            else:
                text += self.extract_text_with_underline(child)
        return text
    
    def clean_whitespace(self, text):
        """
        æ¸…ç†æ–‡æœ¬ä¸­çš„ \n\tï¼šç›´æ¥æ›¿æ›ç‚ºç©ºæ ¼
        å¾ŒçºŒæœƒç”¨ normalize_spaces çµ±ä¸€è™•ç†ç©ºæ ¼
        """
        # å°‡æ‰€æœ‰ \n\t æ›¿æ›ç‚ºå–®å€‹ç©ºæ ¼
        text = re.sub(r'[\n\t]+', ' ', text)
        return text
    
    def normalize_spaces(self, text):
        """
        æ¨™æº–åŒ–ç©ºæ ¼ï¼š
        - ä¿ç•™ï¼šå­—æ¯å’Œå­—æ¯ä¹‹é–“çš„ç©ºæ ¼
        - ç§»é™¤ï¼šæ¼¢å­—å’Œæ¼¢å­—ä¹‹é–“çš„ç©ºæ ¼
        - ç§»é™¤ï¼šæ¼¢å­—å’Œå­—æ¯ä¹‹é–“çš„ç©ºæ ¼
        
        å­—æ¯å®šç¾©ï¼šæ‹‰ä¸å­—æ¯ï¼ˆå«è®ŠéŸ³ç¬¦è™Ÿï¼‰ã€æ•¸å­—ã€åŠå½¢æ¨™é»
        æ¼¢å­—å®šç¾©ï¼šCJKå­—ç¬¦ã€å…¨å½¢æ¨™é»ã€å‡åç­‰
        """
        import unicodedata
        
        def is_latin_like(char):
            """åˆ¤æ–·æ˜¯å¦ç‚ºæ‹‰ä¸å­—æ¯é¡ï¼ˆåŒ…æ‹¬æ•¸å­—ã€åŠå½¢æ¨™é»ã€çµ„åˆç¬¦è™Ÿï¼‰"""
            if not char or char == ' ':
                return False
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºçµ„åˆç”¨è®ŠéŸ³ç¬¦è™Ÿï¼ˆCombining Diacritical Marksï¼‰
            # Unicode ç¯„åœï¼šU+0300-U+036F
            code = ord(char)
            if 0x0300 <= code <= 0x036F:
                return True
            
            try:
                name = unicodedata.name(char, '')
                # æ‹‰ä¸å­—æ¯ï¼ˆå«è®ŠéŸ³ç¬¦è™Ÿï¼‰
                if 'LATIN' in name:
                    return True
                # é¡å¤–æª¢æŸ¥æ˜¯å¦åŒ…å« COMBINING
                if 'COMBINING' in name:
                    return True
            except:
                pass
            # æ•¸å­—å’ŒåŠå½¢æ¨™é»
            if char in '0123456789.,;:!?-()[]{}\'\"':
                return True
            # æª¢æŸ¥æ˜¯å¦ç‚º ASCII å­—æ¯
            if char.isascii() and char.isalpha():
                return True
            return False
        
        def is_cjk_like(char):
            """åˆ¤æ–·æ˜¯å¦ç‚ºCJKå­—ç¬¦é¡ï¼ˆæ¼¢å­—ã€å‡åã€å…¨å½¢æ¨™é»ç­‰ï¼‰"""
            if not char or char == ' ':
                return False
            try:
                name = unicodedata.name(char, '')
                # CJKå­—ç¬¦
                if 'CJK' in name:
                    return True
                # å‡å
                if 'HIRAGANA' in name or 'KATAKANA' in name:
                    return True
            except:
                pass
            # å…¨å½¢æ¨™é»
            if char in 'ï¼Œã€‚ã€ï¼ï¼Ÿï¼›ï¼šã€Œã€ã€ã€ï¼ˆï¼‰ã€ã€‘â€¦â€”':
                return True
            # æª¢æŸ¥ Unicode ç¯„åœ
            code = ord(char)
            # CJK çµ±ä¸€è¡¨æ„æ–‡å­—
            if 0x4E00 <= code <= 0x9FFF:
                return True
            # å‡å
            if 0x3040 <= code <= 0x30FF:
                return True
            return False
        
        # è™•ç†ç©ºæ ¼
        result = []
        i = 0
        while i < len(text):
            char = text[i]
            
            if char == ' ':
                # æª¢æŸ¥å‰å¾Œå­—ç¬¦
                before = text[i-1] if i > 0 else ''
                after = text[i+1] if i < len(text)-1 else ''
                
                before_is_latin = is_latin_like(before)
                after_is_latin = is_latin_like(after)
                
                # åªä¿ç•™å­—æ¯å’Œå­—æ¯ä¹‹é–“çš„ç©ºæ ¼
                if before_is_latin and after_is_latin:
                    result.append(' ')
                # å¦å‰‡è·³éé€™å€‹ç©ºæ ¼
            else:
                result.append(char)
            
            i += 1
        
        return ''.join(result)
    
    def clean_pronunciation(self, pron):
        """æ¸…ç†æ‹¼éŸ³"""
        # å…ˆæ¸…ç† \n\t
        pron = self.clean_whitespace(pron)
        
        # ç§»é™¤å¤šé¤˜ç©ºç™½
        pron = re.sub(r'\s+', ' ', pron).strip()
        
        # ç§»é™¤éæ‹¼éŸ³å­—ç¬¦ï¼ˆä¿ç•™å­—æ¯ã€è²èª¿ç¬¦è™Ÿã€ç©ºæ ¼ã€ä¸‹åŠƒç·šï¼‰
        # ç§»é™¤ --ã€ï¼ç­‰ç¬¦è™Ÿ
        pron = re.sub(r'-{2,}', '', pron)  # ç§»é™¤--
        pron = re.sub(r'[ï¼/]', '', pron)  # ç§»é™¤æ–œç·š
        
        # å†æ¬¡æ¸…ç†ç©ºç™½
        pron = re.sub(r'\s+', ' ', pron).strip()
        
        return pron
    
    # def create_searchable_pronunciation(self, pron):
    #     """å‰µå»ºå¯æœå°‹çš„æ‹¼éŸ³ï¼ˆç„¡è²èª¿ï¼‰"""
    #     # ç§»é™¤è²èª¿ç¬¦è™Ÿ
    #     tone_map = {
    #         'Ä': 'a', 'Ã¡': 'a', 'Ç': 'a', 'Ã ': 'a',
    #         'Ä“': 'e', 'Ã©': 'e', 'Ä›': 'e', 'Ã¨': 'e',
    #         'Ä«': 'i', 'Ã­': 'i', 'Ç': 'i', 'Ã¬': 'i',
    #         'Å': 'o', 'Ã³': 'o', 'Ç’': 'o', 'Ã²': 'o',
    #         'Å«': 'u', 'Ãº': 'u', 'Ç”': 'u', 'Ã¹': 'u',
    #         'Å„': 'n', 'Åˆ': 'n', 'Ç¹': 'n',
    #         'á¸¿': 'm', 'Åˆ': 'n',
    #     }
    #     
    #     searchable = pron
    #     for accented, plain in tone_map.items():
    #         searchable = searchable.replace(accented, plain)
    #     
    #     # ç§»é™¤çµ„åˆç”¨ä¸‹åŠƒç·š
    #     searchable = searchable.replace('\u0331', '')
    #     
    #     return searchable.lower()
    
    def split_by_marker(self, text):
        """
        ä¾æ“šæ¨™è¨˜ç¬¦è™Ÿåˆ†å‰²é‡‹ç¾©å’Œä¾‹å¥
        æ¨™è¨˜åŒ…æ‹¬ï¼š
        - ğŸ“Œ (ä¾‹å¥æ¨™è¨˜ï¼ŒåŸ U+E738)
        - âš« (ä¾‹å¥æ¨™è¨˜)
        - â‘ â‘¡â‘¢... (å®šç¾©ç·¨è™Ÿï¼ŒåŸ U+E897-U+E8A9)
        - â¶â·â¸... (å®šç¾©ç·¨è™Ÿï¼ŒåŸ U+F08C-U+F095)
        - åˆã€å† (å¼•ç”³ç¾©æ¨™è¨˜)
        """
        examples = []
        main_def = text
        
        # å„ªå…ˆè™•ç† ğŸ“Œ ä¾‹å¥æ¨™è¨˜
        if 'ğŸ“Œ' in text:
            parts = text.split('ğŸ“Œ')
            main_def = parts[0].strip()
            examples = [s.strip() for s in parts[1:] if s.strip()]
            return main_def, examples
        
        # è™•ç† âš« æ¨™è¨˜
        if 'âš«' in text:
            parts = text.split('âš«')
            main_def = parts[0].strip()
            examples = [s.strip() for s in parts[1:] if s.strip()]
            return main_def, examples
        
        # å¦‚æœæ²’æœ‰æ˜ç¢ºæ¨™è¨˜ï¼Œå˜—è©¦ç”¨å…¶ä»–æ¨¡å¼è­˜åˆ¥ä¾‹å¥
        # ä¾‹å¥ç‰¹å¾µï¼šåŒ…å«ã€Œï¼ã€ã€Œï¼Ÿã€ã€Œåš’ã€ç­‰å£èªæ¨™è¨˜
        sentences = text.split('ã€‚')
        
        if len(sentences) > 1:
            last_sentence = sentences[-1].strip()
            if last_sentence and any(marker in last_sentence for marker in ['ï¼', 'ï¼Ÿ', 'åš’', 'å•¦', 'å“¦', 'å‘¢']):
                main_def = 'ã€‚'.join(sentences[:-1]) + 'ã€‚'
                examples = [last_sentence]
        
        return main_def.strip(), examples
    
    def split_definitions(self, text):
        """
        åˆ†å‰²å¤šç¾©è©çš„å¤šå€‹å®šç¾©ï¼Œä¿ç•™ marker è³‡è¨Š
        å®šç¾©ç·¨è™Ÿæ¨™è¨˜ï¼šâ‘ â‘¡â‘¢...â‘² (åŸ U+E897-U+E8A9)
        ç‰¹æ®Šç·¨è™Ÿæ¨™è¨˜ï¼šâ¶â·â¸...â¿ (åŸ U+F08C-U+F095)
        
        è¿”å›æ ¼å¼ï¼š[{'marker': 'â‘ ', 'text': '...'}, ...]
        """
        # å®šç¾©ç·¨è™Ÿå­—ç¬¦é›†
        circled_numbers = 'â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²'
        filled_circles = 'â¶â·â¸â¹âºâ»â¼â½â¾â¿'
        all_markers = circled_numbers + filled_circles
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å®šç¾©ç·¨è™Ÿ
        has_markers = any(marker in text for marker in all_markers)
        
        if not has_markers:
            # æ²’æœ‰ç·¨è™Ÿï¼Œè¿”å›å–®ä¸€å®šç¾©ï¼ˆmarker ç‚º Noneï¼‰
            return [{'marker': None, 'text': text}]
        
        # ç”¨æ­£å‰‡è¡¨é”å¼åˆ†å‰²ï¼Œä¿ç•™æ¨™è¨˜
        pattern = f'([{all_markers}])'
        parts = re.split(pattern, text)
        
        definitions = []
        current_marker = None
        current_text = ""
        
        for part in parts:
            if part in all_markers:
                # å„²å­˜å‰ä¸€æ®µ
                if current_text.strip():
                    definitions.append({
                        'marker': current_marker,
                        'text': current_text.strip()
                    })
                current_marker = part
                current_text = ""
            else:
                current_text += part
        
        # ä¿å­˜æœ€å¾Œä¸€æ®µ
        if current_text.strip():
            definitions.append({
                'marker': current_marker,
                'text': current_text.strip()
            })
        
        return definitions if definitions else [{'marker': None, 'text': text}]
    
    def extract_inline_notes(self, text):
        """
        è¨˜éŒ„æ‹¬è™Ÿå…§çš„è¨»é‡‹ï¼Œä½†ä¸å¾ text ä¸­ç§»é™¤
        è¿”å›ï¼š(åŸæ–‡text, inline_notesåˆ—è¡¨)
        """
        # æå–æ‰€æœ‰æ‹¬è™Ÿå…§å®¹
        pattern_paren = r'[ï¼ˆ(]([^ï¼‰)]+)[ï¼‰)]'
        inline_notes = re.findall(pattern_paren, text)
        
        # text ä¿æŒä¸è®Š
        return text, inline_notes
    
    def is_syllable_intro(self, para):
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºéŸ³ç¯€ä»‹ç´¹
        ç‰¹å¾µï¼š
        1. ä»¥å°å­—é«” (size="2" æˆ– 11pt) çš„å–®å­—æ¯/éŸ³ç¯€é–‹é ­
        2. åŒ…å«ã€Œå®¢èªã€ã€Œå–®å…ƒéŸ³ã€ã€ŒéŸ»æ¯ã€ç­‰é—œéµè©
        3. å…§å®¹è¼ƒé•·ï¼ˆé€šå¸¸è¶…é50å­—ï¼‰
        """
        text = para.get_text().strip()
        
        # å¤ªçŸ­ä¸å¯èƒ½æ˜¯éŸ³ç¯€ä»‹ç´¹
        if len(text) < 50:
            return False
        
        # æª¢æŸ¥ç¬¬ä¸€å€‹å­—é«”æ¨™ç±¤
        first_font = para.find('font', face=re.compile(r'Times New Roman'))
        if not first_font:
            return False
        
        size = first_font.get('size', '2')
        if size not in ['2', '11pt']:
            return False
        
        first_text = first_font.get_text().strip()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå–®å­—æ¯æˆ–çŸ­éŸ³ç¯€
        if len(first_text) > 5:
            return False
        
        # æª¢æŸ¥é—œéµè©
        keywords = ['å®¢èª', 'å–®å…ƒéŸ³', 'éŸ»æ¯', 'éŸ»å°¾', 'è²æ¯', 'åœ‹éš›éŸ³æ¨™', 'æ³¨éŸ³ç¬¦è™Ÿ']
        if any(keyword in text for keyword in keywords):
            return True
        
        return False
    
    def is_headword(self, para):
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºä¸»è©æ¢
        ç‰¹å¾µï¼š
        1. å¤§å­—é«” (size="4" æˆ– 14pt)
        2. æ–œé«” + ç²—é«” (<i><b>)
        3. Times New Romanå­—é«”ï¼ˆæ‹¼éŸ³ï¼‰
        4. å¾Œæ¥è¯åº·ä¸­é»‘é«”çš„æ¼¢å­—åˆ—è¡¨
        """
        # å°‹æ‰¾å¤§å­—é«”çš„æ–œé«”ç²—é«”æ‹¼éŸ³
        for font_tag in para.find_all('font', size=re.compile(r'^(4|14pt)$')):
            if font_tag.find('i') and font_tag.find('b'):
                # æª¢æŸ¥å­—é«”æ˜¯å¦ç‚º Times New Roman
                face = font_tag.get('face', '')
                if 'Times New Roman' in face or not face:
                    return True
        return False
    
    def is_character_entry(self, para):
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºå–®å­—è©æ¢
        ç‰¹å¾µï¼š
        1. ä»¥ç²—é«”å–®å€‹æ¼¢å­—é–‹é ­ <b>X</b>
        2. å¾Œæ¥å°å­—æ‹¬è™Ÿ <font size="1">(...)</font>
        
        æ³¨æ„ï¼šä¸æª¢æŸ¥æ‹¬è™Ÿå…§å®¹ï¼Œå› ç‚ºä¸¦éæ‰€æœ‰å­—éƒ½æœ‰å¤å­—å…¸è³‡è¨Š
        ï¼ˆå¦‚ã€Œæ°°ã€åªæœ‰ã€ŒåŒ—äº¬è©±å€Ÿç”¨å­—ï¼Œå®¢èªæ²¿ç”¨ã€ï¼‰
        """
        text = para.get_text().strip()
        
        # å°‹æ‰¾æ®µè½é–‹é ­çš„ç²—é«”æ¨™ç±¤
        first_bold = None
        for child in para.children:
            if isinstance(child, NavigableString):
                if child.strip():
                    break
            elif child.name == 'b' or (child.name == 'font' and child.find('b')):
                first_bold = child
                break
            elif child.name == 'font':
                first_bold_in_font = child.find('b')
                if first_bold_in_font:
                    first_bold = first_bold_in_font
                    break
        
        if not first_bold:
            return False
        
        # æª¢æŸ¥ç²—é«”å…§å®¹æ˜¯å¦ç‚ºå–®å€‹æ¼¢å­—
        bold_text = first_bold.get_text().strip()
        if len(bold_text) != 1:
            return False
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å°å­—æ‹¬è™Ÿ
        small_fonts = para.find_all('font', size=re.compile(r'^(1|8pt)$'))
        if not small_fonts:
            return False
        
        # åªè¦æœ‰ç²—é«”å–®å­— + å°å­—é«”æ¨™ç±¤ï¼Œå°±èªç‚ºæ˜¯ character entry
        # ä¸å†æª¢æŸ¥æ‹¬è™Ÿå…§å®¹æ˜¯å¦åŒ…å«ç‰¹å®šé—œéµè©
        return True
    
    def parse_syllable_intro(self, para):
        """
        è§£æéŸ³ç¯€ä»‹ç´¹
        è¿”å›æ ¼å¼ï¼š
        {
            'type': 'syllable_intro',
            'syllable': 'a',
            'description': 'å®¢èªä¸ƒå€‹å–®å…ƒéŸ³...',
            'sections': [
                {'marker': 'â‘ ', 'text': '...', 'examples': [...]},
                ...
            ]
        }
        """
        # æå–éŸ³ç¯€ï¼ˆç¬¬ä¸€å€‹Times New Romanå­—é«”ï¼‰
        first_font = para.find('font', face=re.compile(r'Times New Roman'))
        if not first_font:
            return None
        
        syllable = first_font.get_text().strip()
        
        # æå–å®Œæ•´æè¿°æ–‡æœ¬
        full_text = para.get_text().strip()
        
        # ç§»é™¤é–‹é ­çš„éŸ³ç¯€éƒ¨åˆ†
        description = full_text.replace(syllable, '', 1).strip()
        
        # åˆ†å‰²ç‚ºä¸åŒæ®µè½ï¼ˆæ ¹æ“š â‘ â‘¡â‘¢ æ¨™è¨˜ï¼‰
        section_parts = self.split_definitions(description)
        
        sections = []
        for part in section_parts:
            text = part['text']
            marker = part['marker']
            
            # æå–ä¾‹å¥
            main_text, examples = self.split_by_marker(text)
            
            sections.append({
                'marker': marker,
                'text': main_text,
                'examples': examples
            })
        
        entry = {
            'id': str(self.current_id),
            'type': 'syllable_intro',
            'syllable': syllable,
            'description': sections[0]['text'] if sections and not sections[0]['marker'] else description,
            'sections': sections if len(sections) > 1 or sections[0].get('marker') else []
        }
        
        self.current_id += 1
        return entry
    
    def parse_headword(self, para):
        """
        è§£æä¸»è©æ¢
        è¿”å›æ ¼å¼ï¼š
        {
            'type': 'headword',
            'pronunciation': 'Ä',
            'characters': ['é˜¿', 'å•Š'],
            'create_character_entries': True  # æ¨™è¨˜éœ€è¦ç‚ºæ¯å€‹å­—ç¬¦å‰µå»ºentry
        }
        """
        # æå–æ‹¼éŸ³ï¼ˆå¤§å­—é«”æ–œé«”ç²—é«”ï¼‰- å¯èƒ½è·¨è¶Šå¤šå€‹ font æ¨™ç±¤
        pronunciation_parts = []
        for font_tag in para.find_all('font', size=re.compile(r'^(4|14pt)$')):
            if font_tag.find('i') and font_tag.find('b'):
                part = self.extract_text_with_underline(font_tag)
                pronunciation_parts.append(part)
        
        if not pronunciation_parts:
            return None
        
        # åˆä½µæ‰€æœ‰éƒ¨åˆ†ï¼ˆè™•ç†è·¨å¤šå€‹ font æ¨™ç±¤çš„æ‹¼éŸ³ï¼Œå¦‚ p + aÌ±ï¼‰
        pronunciation = ''.join(pronunciation_parts)
        pronunciation = self.clean_pronunciation(pronunciation)
        
        # æå–æ¼¢å­—åˆ—è¡¨ï¼ˆè¯åº·ä¸­é»‘é«”æˆ–è¯åº·ç²—é»‘é«”ï¼‰
        characters = []
        for font_tag in para.find_all('font', face=re.compile(r'è¯åº·.*é»‘é«”')):
            bold_tag = font_tag.find('b')
            if bold_tag:
                chars = bold_tag.get_text().strip()
                # å°‡é€£çºŒæ¼¢å­—æ‹†åˆ†ç‚ºå–®å€‹å­—ç¬¦
                characters.extend(list(chars))
        
        # å»é‡ä½†ä¿æŒé †åº
        seen = set()
        characters = [c for c in characters if c not in seen and not seen.add(c)]
        
        entry = {
            'id': str(self.current_id),
            'type': 'headword',
            'pronunciation': {
                'original': pronunciation
            },
            'characters': characters,
            'create_character_entries': True  # æ¨™è¨˜
        }
        
        self.current_id += 1
        return entry
    
    def find_recent_character_entry(self, character, pronunciation):
        """
        æŸ¥æ‰¾æœ€è¿‘æ·»åŠ çš„åŒ¹é…å­—ç¬¦entryï¼ˆå¾å¾Œå¾€å‰æ‰¾ï¼‰
        åªæŸ¥æ‰¾æ²’æœ‰classical_notesçš„åŸºç¤entryï¼ˆå³å¾headwordå‰µå»ºçš„ï¼‰
        """
        # æ¨™æº–åŒ–pronunciationé€²è¡Œæ¯”è¼ƒï¼ˆåªæ¯”è¼ƒå­—ç¬¦ä¸²å€¼ï¼‰
        search_pron = pronunciation if isinstance(pronunciation, str) else pronunciation
        
        for i in range(len(self.entries) - 1, -1, -1):
            entry = self.entries[i]
            if entry['type'] != 'character':
                continue
            
            if entry['character'] != character:
                continue
            
            # æ¯”è¼ƒç™¼éŸ³ï¼ˆæ–°çµæ§‹éƒ½æ˜¯å­—å…¸ï¼‰
            entry_pron_dict = entry.get('pronunciation', {})
            if isinstance(entry_pron_dict, dict):
                entry_pron = entry_pron_dict.get('original', '')
            else:
                # å‘å¾Œå…¼å®¹ï¼ˆèˆŠæ•¸æ“šå¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼‰
                entry_pron = entry_pron_dict
            
            if entry_pron == search_pron:
                # åªæ›´æ–°æ²’æœ‰è©³ç´°è³‡è¨Šçš„entryï¼ˆå¾headwordå‰µå»ºçš„åŸºç¤entryï¼‰
                if not entry.get('classical_notes'):
                    return i
        
        return None
    
    def extract_reading_notes(self, text):
        """
        æå–ã€Œè®€éŸ³ä½œã€å’Œã€ŒèªéŸ³ä½œã€è³‡è¨Š
        è¿”å›ï¼š(literary_reading, vernacular_reading)
        """
        literary = None
        vernacular = None
        
        # æå–ã€Œè®€éŸ³ä½œã€ï¼ˆè¡¨ç¤ºç›®å‰æ˜¯ç™½è®€ï¼ŒXXXæ˜¯æ–‡è®€ï¼‰
        literary_match = re.search(r'è®€éŸ³ä½œ[ã€Œã€]([^ã€ã€]+)[ã€ã€]', text)
        if literary_match:
            literary = literary_match.group(1).strip()
        
        # æå–ã€ŒèªéŸ³ä½œã€ï¼ˆè¡¨ç¤ºç›®å‰æ˜¯æ–‡è®€ï¼ŒXXXæ˜¯ç™½è®€ï¼‰
        vernacular_match = re.search(r'èªéŸ³ä½œ[ã€Œã€]([^ã€ã€]+)[ã€ã€]', text)
        if vernacular_match:
            vernacular = vernacular_match.group(1).strip()
        
        return literary, vernacular
    
    def parse_character_entry(self, para):
        """
        è§£æå–®å­—è©æ¢
        è¿”å›æ ¼å¼ï¼š
        {
            'type': 'character',
            'pronunciation': 'Ä',
            'character': 'é˜¿',
            'classical_notes': 'æºè‡ªåŒ—äº¬è©±éŸ³',
            'literary_reading': None,
            'vernacular_reading': None,
            'definitions': [...]
        }
        """
        # æå–å–®å€‹æ¼¢å­—ï¼ˆç¬¬ä¸€å€‹ç²—é«”ï¼‰
        character = ""
        first_bold = None
        for child in para.children:
            if isinstance(child, NavigableString):
                if child.strip():
                    break
            elif child.name == 'b':
                first_bold = child
                break
            elif child.name == 'font':
                first_bold = child.find('b')
                if first_bold:
                    break
        
        if not first_bold:
            return None
        
        character = first_bold.get_text().strip()
        
        # æå–æ‹¬è™Ÿå…§çš„å¤å­—å…¸è³‡è¨Š
        # å¾æ•´å€‹æ®µè½æ–‡æœ¬ä¸­æå–ï¼ˆå› ç‚ºæ‹¬è™Ÿå¯èƒ½åœ¨ä¸åŒçš„fontæ¨™ç±¤ä¸­ï¼‰
        full_text = para.get_text()
        classical_notes = None
        
        # å°‹æ‰¾ç²—é«”å­—å¾Œé¢çš„æ‹¬è™Ÿå…§å®¹
        # å…ˆå®šä½åˆ°å­—ç¬¦ä½ç½®ï¼Œç„¶å¾Œå°‹æ‰¾å…¶å¾Œçš„æ‹¬è™Ÿ
        char_pos = full_text.find(character)
        if char_pos != -1:
            # å¾å­—ç¬¦å¾Œé¢é–‹å§‹å°‹æ‰¾æ‹¬è™Ÿ
            text_after_char = full_text[char_pos + len(character):]
            # åŒ¹é…æ‹¬è™Ÿå…§å®¹ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ‹¬è™Ÿï¼‰
            match = re.search(r'[ï¼ˆ(]([^ï¼‰)]+)[ï¼‰)]', text_after_char)
            if match:
                classical_notes = match.group(1).strip()
        
        # æå–è®€éŸ³ä½œ/èªéŸ³ä½œ
        literary_reading, vernacular_reading = self.extract_reading_notes(full_text)
        
        # TODO: æå–å®šç¾©ï¼ˆåœ¨å¾ŒçºŒæ®µè½ä¸­ï¼Œä»¥ â‘ â‘¡â‘¢ æˆ–ç¸®æ’æ¨™è¨˜é–‹å§‹ï¼‰
        # é€™éƒ¨åˆ†éœ€è¦åœ¨ä¸»parseå‡½æ•¸ä¸­è™•ç†ä¸Šä¸‹æ–‡é—œä¿‚
        
        entry = {
            'id': None,  # ID åœ¨ä¸»å‡½æ•¸ä¸­è¨­ç½®
            'type': 'character',
            'pronunciation': None,  # åœ¨ä¸»å‡½æ•¸ä¸­è¨­ç½®ç‚ºå­—å…¸çµæ§‹
            'character': character,
            'classical_notes': classical_notes if classical_notes else None,
            'literary_reading': literary_reading,
            'vernacular_reading': vernacular_reading,
            'definitions': []  # éœ€è¦å¾å¾ŒçºŒæ®µè½æå–
        }
        
        # ä¸åœ¨é€™è£¡å¢åŠ  current_idï¼Œç”±èª¿ç”¨è€…æ±ºå®š
        return entry
    
    def is_cjk_char(self, char):
        """
        åˆ¤æ–·æ˜¯å¦ç‚º CJK å­—ç¬¦ï¼ˆæ¼¢å­—ï¼‰
        åŒ…å«æ‰€æœ‰ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—åŠç›¸é—œå€åŸŸ
        """
        if not char:
            return False
        
        code = ord(char)
        
        # å®Œæ•´çš„ CJK Unicode ç¯„åœ
        cjk_ranges = [
            (0x3000, 0x303F),    # ä¸­æ—¥éŸ“ç¬¦è™Ÿå’Œæ¨™é»
            (0x3400, 0x4DBF),    # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€A
            (0x4E00, 0x9FFF),    # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—ï¼ˆåŸºæœ¬å€ï¼‰
            (0xE000, 0xF8FF),    # ç§ç”¨å€
            (0xF900, 0xFAFF),    # ä¸­æ—¥éŸ“ç›¸å®¹è¡¨æ„æ–‡å­—
            (0xFE30, 0xFE4F),    # ä¸­æ—¥éŸ“ç›¸å®¹å½¢å¼
            (0x20000, 0x2A6DF),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€B
            (0x2A700, 0x2B73F),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€C
            (0x2B740, 0x2B81F),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€D
            (0x2B820, 0x2CEAF),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€E
            (0x2CEB0, 0x2EBEF),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€F
            (0x2EBF0, 0x2EE5F),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€I
            (0x2F800, 0x2FA1F),  # ä¸­æ—¥éŸ“ç›¸å®¹è¡¨æ„æ–‡å­—è£œå……å€
            (0x30000, 0x3134F),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€G
            (0x31350, 0x323AF),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€H
            (0x323B0, 0x3347F),  # ä¸­æ—¥éŸ“çµ±ä¸€è¡¨æ„æ–‡å­—æ“´å……å€J
        ]
        
        for start, end in cjk_ranges:
            if start <= code <= end:
                return True
        
        return False
    
    def find_first_cjk_char(self, text):
        """
        æ‰¾ç¬¬ä¸€å€‹ CJK å­—ç¬¦ï¼ˆæ¼¢å­—ï¼‰çš„ä½ç½®
        è¿”å›ï¼šä½ç½®ç´¢å¼•ï¼Œå¦‚æœæ²’æœ‰å‰‡è¿”å› None
        """
        for i, char in enumerate(text):
            if self.is_cjk_char(char):
                return i
        return None
    
    def is_definition_para(self, para):
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºå®šç¾©æ®µè½
        ç‰¹å¾µï¼š
        1. æœ‰ç¸®æ’ (margin-left)
        2. ä»¥ â‘ â‘¡â‘¢ ç­‰æ¨™è¨˜é–‹é ­
        """
        style = para.get('style', '')
        
        # æª¢æŸ¥ç¸®æ’
        if 'margin-left' in style or 'text-indent' in style:
            return True
        
        # æª¢æŸ¥æ˜¯å¦ä»¥ç·¨è™Ÿæ¨™è¨˜é–‹é ­
        text = para.get_text().strip()
        circled_numbers = 'â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²'
        filled_circles = 'â¶â·â¸â¹âºâ»â¼â½â¾â¿'
        
        if text and text[0] in (circled_numbers + filled_circles):
            return True
        
        return False
    
    def parse_compound_entry(self, para):
        """
        è§£æè¤‡åˆè©æ¢ï¼ˆå¤šå­—è©ï¼‰
        æ ¼å¼ï¼š
        1. æ‹¼éŸ³ æ¼¢å­—ï¼é‡‹ç¾©
        2. æ‹¼éŸ³ â‘ é‡‹ç¾©ï¼ˆç„¡æ¼¢å­—å¤–ä¾†èªï¼‰
        3. æ‹¼éŸ³ æ¼¢å­—å®šç¾©ï¼ˆç„¡ç­‰è™Ÿï¼Œå¦‚ï¼šsÄ ku lÃ  æ«»èŠ±ï¼Œæºè‡ª...ï¼‰
        
        æ”¹é€²ç­–ç•¥ï¼š
        - ä½¿ç”¨å®Œæ•´æ–‡æœ¬æå–ï¼ˆè€Œéåªå–ç¬¬ä¸€å€‹ font æ¨™ç±¤ï¼‰
        - æ­£ç¢ºè™•ç†è·¨å¤šå€‹æ¨™ç±¤çš„æ‹¼éŸ³ï¼ˆå¦‚ sÄ bbÃ¬ siÌ±iï¼‰
        - åŸºæ–¼ç¬¬ä¸€å€‹æ¼¢å­—ä½ç½®ä¾†åˆ†å‰²æ‹¼éŸ³å’Œ headword
        """
        # 1. ä½¿ç”¨ extract_text_with_underline æå–å®Œæ•´æ®µè½æ–‡æœ¬ï¼ˆè™•ç†ä¸‹åŠƒç·šï¼‰
        full_text = self.extract_text_with_underline(para)
        full_text = self.clean_whitespace(full_text)
        
        # è·³éå¤ªçŸ­çš„æ®µè½
        if len(full_text.strip()) < 5:
            return None
        
        # 2. æª¢æŸ¥æ˜¯å¦åŒ…å«æ‹¼éŸ³ï¼ˆæ‹‰ä¸å­—æ¯ï¼‰
        has_latin = any(c.isalpha() or c in 'ÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Å„ÅˆÇ¹á¸¿\u0331' for c in full_text)
        if not has_latin:
            return None
        
        # 3. æ‰¾ç¬¬ä¸€å€‹ CJK å­—ç¬¦ï¼ˆæ¼¢å­—ï¼‰çš„ä½ç½®
        first_cjk_pos = self.find_first_cjk_char(full_text)
        
        # 4. åˆ¤æ–·è©æ¢æ ¼å¼ä¸¦åˆ†å‰²
        pronunciation = ""
        headword = ""
        definition_text = ""
        
        if 'ï¼' in full_text or '=' in full_text:
            # æ ¼å¼1ï¼šæœ‰ç­‰è™Ÿçš„è©æ¢
            # ä¾‹ï¼šzá»¥t sÃ­ æ¤Šæ­»ï¼...
            # ä¾‹ï¼šsÄ ku lÃ  ha sÄ ku lÃ  è¦ï¼...
            full_text = full_text.replace('=', 'ï¼')
            
            parts = full_text.split('ï¼', 1)
            if len(parts) != 2:
                return None
            
            before_eq = parts[0].strip()
            definition_text = parts[1].strip()
            
            if first_cjk_pos is not None and first_cjk_pos < len(before_eq):
                # æœ‰æ¼¢å­—åœ¨ç­‰è™Ÿå‰
                # ä½¿ç”¨ã€Œå–®ä½æ•¸ã€åˆ†ææ–¹æ³•ï¼šéŸ³ç¯€+æ¼¢å­—çš„ç¸½æ•¸é™¤ä»¥2
                # ä¾‹å¦‚ï¼šsÄ ku lÃ  ha sÄ ku lÃ è¦
                # 7å€‹éŸ³ç¯€ + 1å€‹æ¼¢å­— = 8å€‹å–®ä½ï¼Œé™¤ä»¥2 = 4
                # å‰4å€‹å–®ä½ï¼ˆsÄ ku lÃ  haï¼‰= pronunciation
                # å¾Œ4å€‹å–®ä½ï¼ˆsÄ ku lÃ  è¦ï¼‰= headword
                
                # 1. åˆ†ææ–‡æœ¬ï¼Œæå–æ‰€æœ‰å–®ä½ï¼ˆéŸ³ç¯€å’Œæ¼¢å­—ï¼‰
                units = []  # å–®ä½åˆ—è¡¨ï¼š{'type': 'syllable'/'cjk', 'text': '...', 'start': 0, 'end': 0}
                current_syllable = ""
                syllable_start = 0
                
                for i, char in enumerate(before_eq):
                    if self.is_cjk_char(char):
                        # é‡åˆ°æ¼¢å­—ï¼Œå…ˆä¿å­˜ç•¶å‰éŸ³ç¯€ï¼ˆå¦‚æœæœ‰ï¼‰
                        if current_syllable:
                            units.append({
                                'type': 'syllable',
                                'text': current_syllable,
                                'start': syllable_start,
                                'end': i
                            })
                            current_syllable = ""
                        # æ·»åŠ æ¼¢å­—å–®ä½
                        units.append({
                            'type': 'cjk',
                            'text': char,
                            'start': i,
                            'end': i + 1
                        })
                    elif char.isalpha() or char in 'ÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Å„ÅˆÇ¹á¸¿\u0331':
                        # æ‹‰ä¸å­—æ¯
                        if not current_syllable:
                            syllable_start = i
                        current_syllable += char
                    elif char == ' ':
                        # ç©ºæ ¼åˆ†éš”éŸ³ç¯€
                        if current_syllable:
                            units.append({
                                'type': 'syllable',
                                'text': current_syllable,
                                'start': syllable_start,
                                'end': i
                            })
                            current_syllable = ""
                
                # è™•ç†æœ€å¾Œä¸€å€‹éŸ³ç¯€
                if current_syllable:
                    units.append({
                        'type': 'syllable',
                        'text': current_syllable,
                        'start': syllable_start,
                        'end': len(before_eq)
                    })
                
                # 2. è¨ˆç®—ç¸½å–®ä½æ•¸ä¸¦æ‰¾åˆ†å‰²é»
                total_units = len(units)
                
                if total_units >= 2:
                    # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çµæ§‹ï¼ˆéŸ³ç¯€æ•¸æ˜é¡¯å¤šæ–¼æ¼¢å­—æ•¸ï¼‰
                    num_syllables = sum(1 for u in units if u['type'] == 'syllable')
                    num_cjk = sum(1 for u in units if u['type'] == 'cjk')
                    
                    # å¦‚æœéŸ³ç¯€æ•¸ > æ¼¢å­—æ•¸ * 2ï¼Œå¯èƒ½æœ‰é‡è¤‡çµæ§‹
                    if num_syllables > num_cjk * 2 and num_cjk > 0:
                        # ä½¿ç”¨ã€Œç¸½å–®ä½æ•¸é™¤ä»¥2ã€çš„æ–¹æ³•
                        split_unit_index = total_units // 2
                        
                        # æ‰¾åˆ°åˆ†å‰²ä½ç½®ï¼ˆç¬¬ split_unit_index å€‹å–®ä½çš„çµæŸä½ç½®ï¼‰
                        if split_unit_index < total_units:
                            split_pos = units[split_unit_index - 1]['end']
                            
                            pronunciation = before_eq[:split_pos].strip()
                            headword = before_eq[split_pos:].strip()
                        else:
                            # ä¸æ‡‰è©²åˆ°é€™è£¡ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•
                            last_space = before_eq[:first_cjk_pos].rfind(' ')
                            if last_space != -1:
                                pronunciation = before_eq[:last_space].strip()
                                headword = before_eq[last_space+1:].strip()
                            else:
                                pronunciation = before_eq[:first_cjk_pos].strip()
                                headword = before_eq[first_cjk_pos:].strip()
                    else:
                        # æ²’æœ‰æ˜é¡¯çš„é‡è¤‡çµæ§‹ï¼Œä½¿ç”¨ç°¡å–®æ–¹æ³•ï¼šç¬¬ä¸€å€‹æ¼¢å­—å‰çš„æœ€å¾Œä¸€å€‹ç©ºæ ¼
                        text_before_cjk = before_eq[:first_cjk_pos]
                        last_space = text_before_cjk.rfind(' ')
                        
                        if last_space != -1:
                            pronunciation = before_eq[:last_space].strip()
                            headword = before_eq[last_space+1:].strip()
                        else:
                            # æ²’æœ‰ç©ºæ ¼ï¼Œæ‹‰ä¸å­—æ¯ç›´æ¥é€£è‘—æ¼¢å­—ï¼ˆå¦‚ lÃ è¦ï¼‰
                            # å¾ç¬¬ä¸€å€‹æ¼¢å­—å¾€å‰æ‰¾é€£çºŒçš„æ‹‰ä¸å­—æ¯æ®µ
                            latin_start = first_cjk_pos
                            while latin_start > 0 and (before_eq[latin_start-1].isalpha() or before_eq[latin_start-1] in 'ÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Å„ÅˆÇ¹á¸¿\u0331'):
                                latin_start -= 1
                            
                            if latin_start > 0:
                                pronunciation = before_eq[:latin_start].strip()
                                headword = before_eq[latin_start:].strip()
                            else:
                                # æ•´å€‹ç­‰è™Ÿå‰éƒ½æ˜¯æ‹‰ä¸+æ¼¢å­—
                                pronunciation = ""
                                headword = before_eq
                else:
                    # å–®ä½æ•¸å¤ªå°‘
                    pronunciation = ""
                    headword = before_eq
            else:
                # ç­‰è™Ÿå‰æ²’æœ‰æ¼¢å­—ï¼ˆç´”æ‹‰ä¸å­—æ¯ headwordï¼‰
                # é€™ç¨®æƒ…æ³è¼ƒå°‘è¦‹ï¼Œæš«æ™‚ç•¶ä½œå¤–ä¾†èªè™•ç†
                pronunciation = before_eq
                headword = before_eq
        
        else:
            # æ²’æœ‰ç­‰è™Ÿçš„æƒ…æ³
            circled_numbers = 'â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²'
            filled_circles = 'â¶â·â¸â¹âºâ»â¼â½â¾â¿'
            all_markers = circled_numbers + filled_circles
            
            if full_text and full_text[0] in all_markers:
                # æ ¼å¼2ï¼šæ‹¼éŸ³ â‘ å®šç¾©ï¼ˆç„¡æ¼¢å­—å¤–ä¾†èªï¼Œä»¥ç·¨è™Ÿé–‹é ­ï¼‰
                # éœ€è¦å¾ para ä¸­æå–æ‹¼éŸ³éƒ¨åˆ†
                # æ‰¾ç¬¬ä¸€å€‹å°å­—é«” font ä½œç‚ºæ‹¼éŸ³
                pron_from_font = ""
                for font_tag in para.find_all('font'):
                    size = font_tag.get('size', '')
                    if size in ['2', '11pt', '9pt']:
                        font_text = font_tag.get_text().strip()
                        if font_text and any(c.isalpha() or c in 'ÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Å„ÅˆÇ¹á¸¿' for c in font_text):
                            pron_from_font = self.extract_text_with_underline(font_tag)
                            break
                
                if pron_from_font:
                    pronunciation = self.clean_pronunciation(pron_from_font)
                    headword = pronunciation
                    definition_text = full_text
                else:
                    return None
                
            elif first_cjk_pos is not None:
                # æ ¼å¼3ï¼šæ‹¼éŸ³ æ¼¢å­—ï¼Œå®šç¾©ï¼ˆç„¡ç­‰è™Ÿã€æœ‰æ¼¢å­—ï¼‰
                # ä¾‹ï¼šsÄ ku lÃ  æ«»èŠ±ï¼Œæºè‡ª...
                # ä¾‹ï¼šsÄ bbÃ¬ siÌ±i æ‹›å¾…ã€æ„æ€æ„æ€ã€‚æºè‡ª...
                
                # ç¬¬ä¸€å€‹æ¼¢å­—ä¹‹å‰çš„æ‰€æœ‰å…§å®¹æ˜¯æ‹¼éŸ³
                pronunciation = full_text[:first_cjk_pos].strip()
                pronunciation = self.clean_pronunciation(pronunciation)
                
                # headword ä½¿ç”¨æ‹¼éŸ³ï¼ˆå› ç‚ºæ²’æœ‰æ˜ç¢ºçš„ headword æ¨™è¨˜ï¼‰
                headword = pronunciation
                
                # å®šç¾©æ˜¯å¾ç¬¬ä¸€å€‹æ¼¢å­—é–‹å§‹
                definition_text = full_text[first_cjk_pos:].strip()
            else:
                # ç´”æ‹‰ä¸å­—æ¯ï¼Œç„¡æ¼¢å­—ï¼Œç„¡ç·¨è™Ÿæ¨™è¨˜
                # æª¢æŸ¥æ˜¯å¦åŒ…å«å®šç¾©ç‰¹å¾µ
                has_definition_keywords = any(kw in full_text for kw in ['æºè‡ª', 'åˆä½œ', 'æ„æ€', 'å±¬æ–¼', 'ï¼Œ'])
                
                if has_definition_keywords:
                    # ç•¶ä½œå¤–ä¾†èªè©æ¢
                    # å˜—è©¦å¾æ–‡æœ¬ä¸­åˆ†é›¢æ‹¼éŸ³éƒ¨åˆ†
                    # æ‰¾ç¬¬ä¸€å€‹å°å­—é«” font
                    pron_from_font = ""
                    for font_tag in para.find_all('font'):
                        size = font_tag.get('size', '')
                        if size in ['2', '11pt', '9pt']:
                            font_text = font_tag.get_text().strip()
                            if font_text and any(c.isalpha() or c in 'ÄÃ¡ÇÃ Ä“Ã©Ä›Ã¨Ä«Ã­ÇÃ¬ÅÃ³Ç’Ã²Å«ÃºÇ”Ã¹Å„ÅˆÇ¹á¸¿' for c in font_text):
                                pron_from_font = self.extract_text_with_underline(font_tag)
                                break
                    
                    if pron_from_font:
                        pronunciation = self.clean_pronunciation(pron_from_font)
                        headword = pronunciation
                        # å¾ full_text ä¸­ç§»é™¤æ‹¼éŸ³éƒ¨åˆ†å¾—åˆ°å®šç¾©
                        pron_text_clean = self.clean_whitespace(pron_from_font)
                        definition_text = full_text.replace(pron_text_clean, '', 1).strip()
                    else:
                        return None
                else:
                    # ä¸æ˜¯è©æ¢
                    return None
        
        # 5. é©—è­‰çµæœ
        if not pronunciation or not headword:
            return None
        
        # 6. æ¸…ç†å¤šé¤˜çš„ç©ºç™½
        definition_text = re.sub(r'\s+', ' ', definition_text).strip()
        
        # 7. åˆ†å‰²å¤šå€‹å®šç¾©ï¼ˆå¦‚æœæœ‰ç·¨è™Ÿæ¨™è¨˜ï¼‰
        definition_parts = self.split_definitions(definition_text)
        
        # 8. è™•ç†æ¯å€‹å®šç¾©
        definitions = []
        for def_dict in definition_parts:
            def_text = def_dict['text']
            marker = def_dict['marker']
            
            # åˆ†é›¢ä¸»é‡‹ç¾©å’Œä¾‹å¥
            main_def, examples = self.split_by_marker(def_text)
            
            # æå– inline_notes
            main_def, inline_notes = self.extract_inline_notes(main_def)
            
            definitions.append({
                'marker': marker,
                'text': main_def,
                'examples': examples,
                'inline_notes': inline_notes
            })
        
        # 9. è­˜åˆ¥ä¾†æº
        source = None
        first_def = definition_parts[0]['text'] if definition_parts else definition_text
        if 'æºè‡ª' in first_def:
            source_match = re.search(r'æºè‡ª([^ã€‚ï¼Œ]+)', first_def)
            if source_match:
                source = source_match.group(1).strip()
        
        # 10. è­˜åˆ¥åˆ¥ç¨±
        variants = []
        if 'åˆä½œ' in definition_text:
            variant_matches = re.findall(r'åˆä½œ[ã€Œã€]([^ã€ã€]+)[ã€ã€]', definition_text)
            variants.extend(variant_matches)
        
        # 11. å‰µå»ºè©æ¢è³‡æ–™
        entry = {
            'id': str(self.current_id),
            'type': 'word',
            'pronunciation': {
                'original': pronunciation
            },
            'headword': headword,
            'definitions': definitions,
            'source': source,
            'variants': variants
        }
        
        self.current_id += 1
        return entry
    
    def parse(self, html_path):
        """è§£ææ•´å€‹HTMLæ–‡ä»¶"""
        soup = self.load_html(html_path)
        
        # æ‰¾åˆ°ä¸»è¦å…§å®¹å€åŸŸ
        main_section = soup.find('div', id='TextSection')
        if not main_section:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°ä¸»è¦å…§å®¹å€åŸŸ")
            return []
        
        # éæ­·æ‰€æœ‰æ®µè½
        paragraphs = main_section.find_all('p')
        print(f"æ‰¾åˆ° {len(paragraphs)} å€‹æ®µè½")
        
        current_pronunciation = None  # ç”¨æ–¼è¿½è¹¤ç•¶å‰çš„æ‹¼éŸ³ï¼ˆçµ¦characterç”¨ï¼‰
        pending_character_entry = None  # å¾…è£œå……definitionsçš„character entry
        
        for i, para in enumerate(paragraphs):
            # 1. éŸ³ç¯€ä»‹ç´¹ - å·²ç§»é™¤ï¼Œç•¶ä½œæ™®é€šè©æ¢è™•ç†
            # if self.is_syllable_intro(para):
            #     ...
            
            # 2. ä¸»è©æ¢
            if self.is_headword(para):
                # çµæŸä¹‹å‰çš„character entryè™•ç†
                pending_character_entry = None
                
                entry = self.parse_headword(para)
                if entry:
                    self.entries.append(entry)
                    current_pronunciation = entry['pronunciation']['original']
                    
                    # ç‚ºheadwordä¸­çš„æ¯å€‹å­—ç¬¦å‰µå»ºåŸºç¤character entry
                    if entry.get('create_character_entries'):
                        for char in entry['characters']:
                            char_entry = {
                                'id': str(self.current_id),
                                'type': 'character',
                                'pronunciation': {
                                    'original': current_pronunciation
                                },
                                'character': char,
                                'classical_notes': None,
                                'literary_reading': None,
                                'vernacular_reading': None,
                                'definitions': []
                            }
                            self.entries.append(char_entry)
                            self.current_id += 1
                    
                    if len(self.entries) % 50 == 0:
                        print(f"å·²è§£æ {len(self.entries)} å€‹æ¢ç›®...")
                continue
            
            # 3. å–®å­—è©æ¢ï¼ˆè©³ç´°çš„ï¼Œæœ‰å¤å­—å…¸è³‡è¨Šï¼‰
            if self.is_character_entry(para):
                entry = self.parse_character_entry(para)
                if entry:
                    # è¨­ç½®pronunciationç‚ºå­—å…¸çµæ§‹
                    if current_pronunciation:
                        entry['pronunciation'] = {
                            'original': current_pronunciation
                        }
                    else:
                        entry['pronunciation'] = {
                            'original': ""
                        }
                    
                    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨åŸºç¤entryï¼ˆå¾headwordå‰µå»ºçš„ï¼‰
                    existing_idx = self.find_recent_character_entry(
                        entry['character'], 
                        current_pronunciation
                    )
                    
                    if existing_idx is not None:
                        # æ›´æ–°ç¾æœ‰çš„åŸºç¤entry
                        self.entries[existing_idx].update({
                            'classical_notes': entry['classical_notes'],
                            'literary_reading': entry['literary_reading'],
                            'vernacular_reading': entry['vernacular_reading']
                        })
                        # ä¿ç•™å°é€™å€‹entryçš„å¼•ç”¨ï¼Œä»¥ä¾¿å¾ŒçºŒæ·»åŠ definitions
                        pending_character_entry = self.entries[existing_idx]
                    else:
                        # æ²’æœ‰æ‰¾åˆ°åŸºç¤entryï¼Œå‰µå»ºæ–°çš„ï¼ˆå¯èƒ½æ˜¯æ²’æœ‰headwordçš„ç¨ç«‹å­—ï¼‰
                        entry['id'] = str(self.current_id)
                        self.current_id += 1
                        self.entries.append(entry)
                        pending_character_entry = entry
                    
                    if len(self.entries) % 50 == 0:
                        print(f"å·²è§£æ {len(self.entries)} å€‹æ¢ç›®...")
                continue
            
            # 4. å®šç¾©æ®µè½ï¼ˆå±¬æ–¼å‰ä¸€å€‹character entryï¼‰
            if pending_character_entry and self.is_definition_para(para):
                text = para.get_text().strip()
                text = self.clean_whitespace(text)
                
                # å…ˆæå–é–‹é ­çš„ç·¨è™Ÿæ¨™è¨˜ï¼ˆå¦‚æœæœ‰ï¼‰
                marker = None
                circled_numbers = 'â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²'
                filled_circles = 'â¶â·â¸â¹âºâ»â¼â½â¾â¿'
                all_markers = circled_numbers + filled_circles
                
                if text and text[0] in all_markers:
                    marker = text[0]
                    text = text[1:].strip()  # ç§»é™¤marker
                
                # åˆ†é›¢å®šç¾©å’Œä¾‹å¥
                main_def, examples = self.split_by_marker(text)
                main_def, inline_notes = self.extract_inline_notes(main_def)
                
                pending_character_entry['definitions'].append({
                    'marker': marker,
                    'text': main_def,
                    'examples': examples,
                    'inline_notes': inline_notes
                })
                continue
            
            # 5. å¤šå­—è©æ¢
            entry = self.parse_compound_entry(para)
            if entry:
                # ç•¶é‡åˆ°æ–°çš„word entryæ™‚ï¼ŒçµæŸä¹‹å‰çš„character entryè™•ç†
                pending_character_entry = None
                
                self.entries.append(entry)
                
                # æ¯100å€‹è©æ¢é¡¯ç¤ºé€²åº¦
                if len(self.entries) % 100 == 0:
                    print(f"å·²è§£æ {len(self.entries)} å€‹æ¢ç›®...")
        
        print(f"\nç¸½å…±è§£æå‡º {len(self.entries)} å€‹æ¢ç›®")
        return self.entries
    
    def clean_all_text_fields(self):
        """
        åœ¨ä¿å­˜å‰çµ±ä¸€æ¸…ç†æ‰€æœ‰æ–‡æœ¬æ¬„ä½ä¸­çš„ \\n\\t ä¸¦æ¨™æº–åŒ–ç©ºæ ¼
        éæ­·æ‰€æœ‰ entriesï¼Œæ¸…ç†æ‰€æœ‰å­—ç¬¦ä¸²é¡å‹çš„å€¼
        """
        def clean_value(value):
            """éæ­¸æ¸…ç†å€¼"""
            if isinstance(value, str):
                # å…ˆæ¸…ç† \n\tï¼Œå†æ¨™æº–åŒ–ç©ºæ ¼
                value = self.clean_whitespace(value)
                value = self.normalize_spaces(value)
                return value
            elif isinstance(value, dict):
                return {k: clean_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [clean_value(item) for item in value]
            else:
                return value
        
        print("\næ¸…ç†æ–‡æœ¬ä¸­çš„ \\n\\t ä¸¦æ¨™æº–åŒ–ç©ºæ ¼...")
        for entry in self.entries:
            for key, value in entry.items():
                entry[key] = clean_value(value)
        print("æ¸…ç†å®Œæˆ")
    
    def save_json(self, output_path):
        """ä¿å­˜ç‚ºJSONæ ¼å¼"""
        # åœ¨ä¿å­˜å‰çµ±ä¸€æ¸…ç†æ‰€æœ‰ \n\t
        self.clean_all_text_fields()
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump({
                'total': len(self.entries),
                'entries': self.entries
            }, f, ensure_ascii=False, indent=2)
        print(f"å·²ä¿å­˜åˆ° {output_path}")
    
    def print_sample(self, n=5):
        """é¡¯ç¤ºå‰Nå€‹æ¢ç›®çš„æ¨£æœ¬"""
        print(f"\n=== é¡¯ç¤ºå‰ {n} å€‹æ¢ç›® ===\n")
        for entry in self.entries[:n]:
            entry_type = entry['type']
            print(f"é¡å‹: {entry_type}")
            
            # syllable_intro é¡å‹å·²ç§»é™¤
            # if entry_type == 'syllable_intro':
            #     print(f"éŸ³ç¯€: {entry['syllable']}")
            #     print(f"æè¿°: {entry['description'][:100]}...")
            
            if entry_type == 'headword':
                print(f"æ‹¼éŸ³: {entry['pronunciation']['original']}")
                print(f"æ¼¢å­—: {', '.join(entry['characters'])}")
            
            elif entry_type == 'character':
                pron = entry['pronunciation']
                if isinstance(pron, dict):
                    pron_str = pron.get('original', '')
                else:
                    pron_str = pron  # å‘å¾Œå…¼å®¹
                print(f"æ‹¼éŸ³: {pron_str}")
                print(f"æ¼¢å­—: {entry['character']}")
                if entry['classical_notes']:
                    print(f"å¤å­—å…¸: {entry['classical_notes'][:80]}...")
                if entry['literary_reading']:
                    print(f"æ–‡è®€: {entry['literary_reading']}")
                if entry['vernacular_reading']:
                    print(f"ç™½è®€: {entry['vernacular_reading']}")
                if entry['definitions']:
                    print(f"å®šç¾©æ•¸: {len(entry['definitions'])}")
            
            elif entry_type == 'word':
                print(f"æ‹¼éŸ³: {entry['pronunciation']['original']}")
                print(f"è©æ¢: {entry['headword']}")
                
                # é¡¯ç¤ºæ‰€æœ‰å®šç¾©
                for i, definition in enumerate(entry['definitions'], 1):
                    marker = definition.get('marker')
                    marker_str = f"[{marker}] " if marker else ""
                    
                    if len(entry['definitions']) > 1:
                        print(f"å®šç¾© {i}: {marker_str}{definition['text'][:80]}...")
                    else:
                        print(f"é‡‹ç¾©: {marker_str}{definition['text'][:80]}...")
                    
                    if definition['examples']:
                        print(f"  ä¾‹å¥: {definition['examples'][0][:60]}...")
            
            print("-" * 60)
    
    def print_statistics(self):
        """é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š"""
        print("\n=== çµ±è¨ˆè³‡è¨Š ===")
        print(f"ç¸½æ¢ç›®æ•¸: {len(self.entries)}")
        
        # æŒ‰é¡å‹çµ±è¨ˆ
        type_counts = {}
        for entry in self.entries:
            entry_type = entry['type']
            type_counts[entry_type] = type_counts.get(entry_type, 0) + 1
        
        print("\næŒ‰é¡å‹çµ±è¨ˆ:")
        for entry_type, count in sorted(type_counts.items()):
            percentage = count / len(self.entries) * 100
            print(f"  {entry_type}: {count} ({percentage:.1f}%)")
        
        # è©æ¢çµ±è¨ˆ
        word_entries = [e for e in self.entries if e['type'] == 'word']
        if word_entries:
            with_examples = sum(1 for e in word_entries 
                               if any(d['examples'] for d in e['definitions']))
            print(f"\nå¤šå­—è©æ¢çµ±è¨ˆ:")
            print(f"  ç¸½æ•¸: {len(word_entries)}")
            print(f"  æœ‰ä¾‹å¥: {with_examples} ({with_examples/len(word_entries)*100:.1f}%)")
            
            multi_def = sum(1 for e in word_entries if len(e['definitions']) > 1)
            print(f"  å¤šç¾©è©: {multi_def} ({multi_def/len(word_entries)*100:.1f}%)")
        
        # å–®å­—è©æ¢çµ±è¨ˆ
        char_entries = [e for e in self.entries if e['type'] == 'character']
        if char_entries:
            with_classical = sum(1 for e in char_entries if e.get('classical_notes'))
            with_defs = sum(1 for e in char_entries if e.get('definitions'))
            print(f"\nå–®å­—è©æ¢çµ±è¨ˆ:")
            print(f"  ç¸½æ•¸: {len(char_entries)}")
            print(f"  æœ‰å¤å­—å…¸è¨»: {with_classical} ({with_classical/len(char_entries)*100:.1f}%)")
            print(f"  æœ‰å®šç¾©: {with_defs} ({with_defs/len(char_entries)*100:.1f}%)")


def main():
    """ä¸»ç¨‹å¼"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python hakka_parser_enhanced.py <HTMLæª”æ¡ˆè·¯å¾‘> [è¼¸å‡ºJSONè·¯å¾‘]")
        print("ä¾‹å¦‚: python hakka_parser_enhanced.py æµ·é™¸è…”è¾­å…¸.html output.json")
        sys.exit(1)
    
    html_path = sys.argv[1]
    output_path = sys.argv[2] if len(sys.argv) > 2 else 'hakka_dict_enhanced.json'
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not Path(html_path).exists():
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {html_path}")
        sys.exit(1)
    
    # è§£æ
    parser = HakkaParser()
    entries = parser.parse(html_path)
    
    # é¡¯ç¤ºæ¨£æœ¬
    parser.print_sample(10)
    
    # ä¿å­˜
    parser.save_json(output_path)
    
    # çµ±è¨ˆ
    parser.print_statistics()


if __name__ == '__main__':
    main()